{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import DeepLake\n",
    "\n",
    "import dotenv\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "os.environ['ACTIVELOOP_TOKEN'] = \"YourAPIKey\" # replace with your API key from app.activeloop.ai\n",
    "username = \"rihp\" # replace with your username from app.activeloop.ai\n",
    "projectname = \"autogpt1\" # replace with your project name from app.activeloop.ai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings(disallowed_special=())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !git clone https://github.com/Significant-Gravitas/Auto-GPT # replace any repository of your choice "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "root_dir = './Auto-GPT' # Specify where your data is stored\n",
    "docs = []\n",
    "for dirpath, dirnames, filenames in os.walk(root_dir):\n",
    "    for file in filenames:\n",
    "        try: \n",
    "            loader = TextLoader(os.path.join(dirpath, file), encoding='utf-8')\n",
    "            docs.extend(loader.load_and_split())\n",
    "        except Exception as e: \n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "texts = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "db = DeepLake(dataset_path=f\"hub://{username}/{projectname}\", embedding_function=embeddings, public=True) #dataset would be publicly available\n",
    "db.add_documents(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This dataset can be visualized in Jupyter Notebook by ds.visualize() or at https://app.activeloop.ai/rihp/autogpt1\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\\"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hub://rihp/autogpt1 loaded successfully.\n",
      "\n",
      "Deep Lake Dataset in hub://rihp/autogpt1 already exists, loading from the storage\n",
      "Dataset(path='hub://rihp/autogpt1', read_only=True, tensors=['embedding', 'ids', 'metadata', 'text'])\n",
      "\n",
      "  tensor     htype      shape      dtype  compression\n",
      "  -------   -------    -------    -------  ------- \n",
      " embedding  generic  (920, 1536)  float32   None   \n",
      "    ids      text     (920, 1)      str     None   \n",
      " metadata    json     (920, 1)      str     None   \n",
      "   text      text     (920, 1)      str     None   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " \r"
     ]
    }
   ],
   "source": [
    "db = DeepLake(dataset_path=f\"hub://{username}/{projectname}\", read_only=True, embedding_function=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = db.as_retriever()\n",
    "retriever.search_kwargs['distance_metric'] = 'cos'\n",
    "retriever.search_kwargs['fetch_k'] = 100\n",
    "retriever.search_kwargs['maximal_marginal_relevance'] = True\n",
    "retriever.search_kwargs['k'] = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "model = ChatOpenAI(model_name='gpt-3.5-turbo') # switch to 'gpt-4'\n",
    "qa = ConversationalRetrievalChain.from_llm(model, retriever=retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> **Question**: What tests have been developed for this repository? Check the tests folder for more information. \n",
      "\n",
      "**Answer**: I'm sorry, but I cannot see any mention of a tests folder in the provided context. The context includes some code snippets, a documentation checklist, and a few references to GitHub users, but no mention of a tests folder. \n",
      "\n",
      "-> **Question**: What happens in the memory module of the repository? \n",
      "\n",
      "**Answer**: The memory module of the repository refers to the memory backend that Auto-GPT uses to store and access relevant data for its AI tasks. This module provides different options for memory backends such as local, Pinecone, Redis, Milvus, and Weaviate. Memory pre-seeding is a technique that is mentioned to improve AI accuracy by ingesting relevant data into its memory. Chunks of data are split and added to memory, allowing the AI to access them quickly and generate more accurate responses. The memory module is a crucial component of Auto-GPT as it helps improve its performance in tasks that require remembering and using information over time. \n",
      "\n",
      "-> **Question**: Can you explain how the AutoGPT repository works? \n",
      "\n",
      "**Answer**: Auto-GPT is an experimental open-source application that showcases the capabilities of the GPT-4 language model. The program is driven by GPT-4, which chains together LLM \"thoughts\" to autonomously achieve whatever goal you set. Auto-GPT pushes the boundaries of what is possible with AI.\n",
      "\n",
      "Regarding the memory module, Auto-GPT currently supports Redis as a memory backend, but it can be extended to other databases. The memory module is used to store the conversations Auto-GPT has with users. Redis is an in-memory data structure store that can be used as a database, cache, and message broker. It is highly recommended to run Auto-GPT with `WIPE_REDIS_ON_START=False` to avoid losing memory on startup.\n",
      "\n",
      "Memory pre-seeding is a technique used to improve AI accuracy by ingesting relevant data into its memory. This technique involves splitting chunks of data and adding them to memory, enabling the AI to access them quickly and generate more accurate responses. Memory pre-seeding is useful for large datasets or when specific information needs to be accessed quickly. Examples include ingesting API or GitHub documentation before running Auto-GPT. Memories will be available to the AI immediately as they are ingested, even if ingested while Auto-GPT is running.\n",
      "\n",
      "For other memory backends, Auto-GPT currently forcefully wipes the memory when starting. To ingest data with those memory backends, you can call the `data_ingestion.py` script anytime during an Auto-GPT run.\n",
      "\n",
      "Overall, Auto-GPT is an experimental application, and its memory module is continually being improved to enhance the AI's performance. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "questions = [\n",
    "    \"\"\"What tests have been developed for this repository? Check the tests folder for more information.\"\"\",  \n",
    "    \"What happens in the memory module of the repository?\",\n",
    "    \"Can you explain how the AutoGPT repository works?\",\n",
    "] \n",
    "chat_history = []\n",
    "\n",
    "for question in questions:  \n",
    "    result = qa({\"question\": question, \"chat_history\": chat_history})\n",
    "    chat_history.append((question, result['answer']))\n",
    "    print(f\"-> **Question**: {question} \\n\")\n",
    "    print(f\"**Answer**: {result['answer']} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.3 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
